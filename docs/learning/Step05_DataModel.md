# Step05: ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã¨ã‚¹ã‚­ãƒ¼ãƒè©³ç´°

## ğŸ¯ ã“ã®ç« ã®ç›®æ¨™
PostgreSQLãƒ»Milvusã§ã®ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã€ã‚¹ã‚­ãƒ¼ãƒè©³ç´°ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥ã€ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹

---

## ğŸ“‹ æ¦‚è¦

RAGã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼‰ã¨éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ™ã‚¯ã‚¿ãƒ¼ï¼‰ã‚’åŠ¹ç‡çš„ã«ç®¡ç†ã™ã‚‹ãŸã‚ã€PostgreSQLã¨Milvusã‚’ä½¿ã„åˆ†ã‘ã¦ã„ã¾ã™ã€‚é©åˆ‡ãªã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆã«ã‚ˆã‚Šã€é«˜é€Ÿæ¤œç´¢ã¨æ‹¡å¼µæ€§ã‚’ä¸¡ç«‹ã—ã¾ã™ã€‚

### ğŸ—ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹æˆ

```
ãƒ‡ãƒ¼ã‚¿ä¿å­˜æˆ¦ç•¥
â”œâ”€â”€ PostgreSQL        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ documents     # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸºæœ¬æƒ…å ±
â”‚   â”œâ”€â”€ chunks        # ãƒãƒ£ãƒ³ã‚¯è©³ç´°
â”‚   â”œâ”€â”€ sources       # ã‚½ãƒ¼ã‚¹ç®¡ç†
â”‚   â””â”€â”€ users         # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»èªè¨¼
â”œâ”€â”€ Milvus            # ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ dense_collection    # Dense vectors
â”‚   â”œâ”€â”€ sparse_collection   # Sparse vectors
â”‚   â””â”€â”€ multi_collection    # Multi-vectors
â””â”€â”€ Redis             # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³
    â”œâ”€â”€ search_cache  # æ¤œç´¢çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
    â”œâ”€â”€ embedding_cache # åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    â””â”€â”€ session_store # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³
```

---

## ğŸ—ƒï¸ PostgreSQL ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆ

### 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«

#### `documents` ãƒ†ãƒ¼ãƒ–ãƒ«
```sql
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title VARCHAR(1000) NOT NULL,
    content TEXT NOT NULL,
    source_type VARCHAR(50) NOT NULL,
    source_id VARCHAR(255) NOT NULL,
    source_url TEXT,
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    author VARCHAR(255),
    language CHAR(2) DEFAULT 'ja',
    category VARCHAR(100),
    tags TEXT[], -- PostgreSQLé…åˆ—
    
    -- çµ±è¨ˆæƒ…å ±
    word_count INTEGER DEFAULT 0,
    char_count INTEGER DEFAULT 0,
    chunk_count INTEGER DEFAULT 0,
    
    -- å‡¦ç†çŠ¶æ³
    processing_status VARCHAR(20) DEFAULT 'pending',
    indexing_status VARCHAR(20) DEFAULT 'pending',
    error_message TEXT,
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    indexed_at TIMESTAMP WITH TIME ZONE,
    
    -- åˆ¶ç´„
    CONSTRAINT valid_processing_status 
        CHECK (processing_status IN ('pending', 'processing', 'completed', 'failed')),
    CONSTRAINT valid_indexing_status 
        CHECK (indexing_status IN ('pending', 'indexing', 'completed', 'failed')),
    CONSTRAINT valid_language 
        CHECK (language ~ '^[a-z]{2}$'),
    CONSTRAINT positive_counts 
        CHECK (word_count >= 0 AND char_count >= 0 AND chunk_count >= 0)
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_documents_source ON documents (source_type, source_id);
CREATE INDEX idx_documents_status ON documents (processing_status, indexing_status);
CREATE INDEX idx_documents_created ON documents (created_at DESC);
CREATE INDEX idx_documents_language ON documents (language);
CREATE INDEX idx_documents_category ON documents (category);
CREATE INDEX idx_documents_tags ON documents USING GIN (tags);
CREATE UNIQUE INDEX idx_documents_source_unique ON documents (source_type, source_id);

-- å…¨æ–‡æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_documents_fulltext ON documents 
    USING GIN (to_tsvector('japanese', title || ' ' || content));
```

#### `document_chunks` ãƒ†ãƒ¼ãƒ–ãƒ«
```sql
CREATE TABLE document_chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    
    -- ãƒãƒ£ãƒ³ã‚¯å†…å®¹
    content TEXT NOT NULL,
    chunk_type VARCHAR(50) DEFAULT 'text',
    position INTEGER NOT NULL,
    
    -- æ§‹é€ æƒ…å ±
    section_title VARCHAR(500),
    hierarchy_level INTEGER DEFAULT 1,
    parent_chunk_id UUID REFERENCES document_chunks(id),
    
    -- ã‚µã‚¤ã‚ºæƒ…å ±
    token_count INTEGER DEFAULT 0,
    char_count INTEGER DEFAULT 0,
    
    -- ãƒ™ã‚¯ã‚¿ãƒ¼é–¢é€£
    dense_vector_id VARCHAR(255), -- Milvuså†…ã®ID
    sparse_vector_id VARCHAR(255),
    multi_vector_id VARCHAR(255),
    
    -- å‡¦ç†çŠ¶æ³
    embedding_status VARCHAR(20) DEFAULT 'pending',
    embedding_error TEXT,
    
    -- ã‚¹ã‚³ã‚¢æƒ…å ±ï¼ˆæ¤œç´¢æ™‚ã«æ›´æ–°ï¼‰
    last_search_score FLOAT,
    search_count INTEGER DEFAULT 0,
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    embedded_at TIMESTAMP WITH TIME ZONE,
    
    -- åˆ¶ç´„
    CONSTRAINT valid_embedding_status 
        CHECK (embedding_status IN ('pending', 'processing', 'completed', 'failed')),
    CONSTRAINT positive_position CHECK (position >= 0),
    CONSTRAINT positive_counts 
        CHECK (token_count >= 0 AND char_count >= 0 AND hierarchy_level >= 1)
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_chunks_document ON document_chunks (document_id, position);
CREATE INDEX idx_chunks_embedding_status ON document_chunks (embedding_status);
CREATE INDEX idx_chunks_vector_ids ON document_chunks (dense_vector_id, sparse_vector_id);
CREATE INDEX idx_chunks_hierarchy ON document_chunks (hierarchy_level, section_title);
CREATE INDEX idx_chunks_parent ON document_chunks (parent_chunk_id);

-- å…¨æ–‡æ¤œç´¢
CREATE INDEX idx_chunks_fulltext ON document_chunks 
    USING GIN (to_tsvector('japanese', content));
```

### 2. ã‚½ãƒ¼ã‚¹ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«

#### `sources` ãƒ†ãƒ¼ãƒ–ãƒ«
```sql
CREATE TABLE sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source_type VARCHAR(50) NOT NULL,
    name VARCHAR(255) NOT NULL,
    
    -- æ¥ç¶šæƒ…å ±
    connection_config JSONB NOT NULL,
    credentials_encrypted TEXT,
    
    -- åŒæœŸè¨­å®š
    sync_schedule VARCHAR(100), -- Cronå¼
    last_sync_at TIMESTAMP WITH TIME ZONE,
    next_sync_at TIMESTAMP WITH TIME ZONE,
    sync_status VARCHAR(20) DEFAULT 'pending',
    
    -- çµ±è¨ˆæƒ…å ±
    total_documents INTEGER DEFAULT 0,
    successful_documents INTEGER DEFAULT 0,
    failed_documents INTEGER DEFAULT 0,
    
    -- çŠ¶æ…‹ç®¡ç†
    is_active BOOLEAN DEFAULT true,
    error_count INTEGER DEFAULT 0,
    last_error TEXT,
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    CONSTRAINT valid_sync_status 
        CHECK (sync_status IN ('pending', 'running', 'completed', 'failed'))
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_sources_type ON sources (source_type);
CREATE INDEX idx_sources_active ON sources (is_active);
CREATE INDEX idx_sources_sync_schedule ON sources (next_sync_at) WHERE is_active = true;
CREATE UNIQUE INDEX idx_sources_name_type ON sources (source_type, name);
```

### 3. èªè¨¼ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†

#### `users` ãƒ†ãƒ¼ãƒ–ãƒ«
```sql
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) NOT NULL UNIQUE,
    username VARCHAR(100) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    
    -- ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«
    full_name VARCHAR(255),
    avatar_url TEXT,
    timezone VARCHAR(50) DEFAULT 'Asia/Tokyo',
    
    -- æ¨©é™ãƒ»ãƒ­ãƒ¼ãƒ«
    role VARCHAR(50) DEFAULT 'viewer',
    permissions TEXT[], -- PostgreSQLé…åˆ—
    is_active BOOLEAN DEFAULT true,
    is_verified BOOLEAN DEFAULT false,
    
    -- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
    failed_login_attempts INTEGER DEFAULT 0,
    locked_until TIMESTAMP WITH TIME ZONE,
    last_login_at TIMESTAMP WITH TIME ZONE,
    password_changed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    CONSTRAINT valid_role 
        CHECK (role IN ('viewer', 'editor', 'admin', 'super_admin')),
    CONSTRAINT valid_email 
        CHECK (email ~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$')
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_users_email ON users (email);
CREATE INDEX idx_users_role ON users (role);
CREATE INDEX idx_users_active ON users (is_active);
CREATE INDEX idx_users_permissions ON users USING GIN (permissions);
```

#### `api_keys` ãƒ†ãƒ¼ãƒ–ãƒ«
```sql
CREATE TABLE api_keys (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    
    -- API Keyæƒ…å ±
    key_hash VARCHAR(255) NOT NULL UNIQUE,
    key_prefix VARCHAR(20) NOT NULL, -- è¡¨ç¤ºç”¨ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
    name VARCHAR(255) NOT NULL,
    description TEXT,
    
    -- æ¨©é™
    permissions TEXT[] NOT NULL,
    rate_limit_per_minute INTEGER DEFAULT 100,
    
    -- ä½¿ç”¨çŠ¶æ³
    usage_count INTEGER DEFAULT 0,
    last_used_at TIMESTAMP WITH TIME ZONE,
    
    -- çŠ¶æ…‹
    is_active BOOLEAN DEFAULT true,
    expires_at TIMESTAMP WITH TIME ZONE,
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    CONSTRAINT positive_rate_limit CHECK (rate_limit_per_minute > 0)
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_api_keys_user ON api_keys (user_id);
CREATE INDEX idx_api_keys_active ON api_keys (is_active);
CREATE INDEX idx_api_keys_expires ON api_keys (expires_at) WHERE expires_at IS NOT NULL;
```

---

## ğŸ” Milvus ãƒ™ã‚¯ã‚¿ãƒ¼ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¨­è¨ˆ

### 1. Dense Vector Collection

```python
from pymilvus import CollectionSchema, FieldSchema, DataType

# ã‚¹ã‚­ãƒ¼ãƒå®šç¾©
dense_collection_schema = CollectionSchema(
    fields=[
        FieldSchema(
            name="id",
            dtype=DataType.VARCHAR,
            max_length=255,
            is_primary=True,
            description="ãƒãƒ£ãƒ³ã‚¯IDï¼ˆPostgreSQLã¨é€£æºï¼‰"
        ),
        FieldSchema(
            name="vector",
            dtype=DataType.FLOAT_VECTOR,
            dim=1024,  # BGE-M3ã®Dense Vectoræ¬¡å…ƒ
            description="1024æ¬¡å…ƒDense Vector"
        ),
        FieldSchema(
            name="document_id",
            dtype=DataType.VARCHAR,
            max_length=255,
            description="è¦ªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID"
        ),
        FieldSchema(
            name="source_type",
            dtype=DataType.VARCHAR,
            max_length=50,
            description="ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—"
        ),
        FieldSchema(
            name="language",
            dtype=DataType.VARCHAR,
            max_length=2,
            description="è¨€èªã‚³ãƒ¼ãƒ‰"
        ),
        FieldSchema(
            name="chunk_position",
            dtype=DataType.INT32,
            description="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã®ä½ç½®"
        ),
        FieldSchema(
            name="created_timestamp",
            dtype=DataType.INT64,
            description="ä½œæˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆUnixæ™‚é–“ï¼‰"
        )
    ],
    description="Dense vectors for semantic search",
    enable_dynamic_field=True  # å°†æ¥ã®æ‹¡å¼µã«å¯¾å¿œ
)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®š
dense_index_params = {
    "metric_type": "IP",  # Inner Product
    "index_type": "HNSW",
    "params": {
        "M": 16,              # ã‚°ãƒ©ãƒ•ã®æœ€å¤§æ¥ç¶šæ•°
        "efConstruction": 256  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚ã®æ¢ç´¢å¹…
    }
}
```

### 2. Sparse Vector Collection

```python
# Sparse Vectorç”¨ã‚¹ã‚­ãƒ¼ãƒ
sparse_collection_schema = CollectionSchema(
    fields=[
        FieldSchema(
            name="id",
            dtype=DataType.VARCHAR,
            max_length=255,
            is_primary=True
        ),
        FieldSchema(
            name="vector",
            dtype=DataType.SPARSE_FLOAT_VECTOR,
            description="Sparse Vectorï¼ˆèªå½™é‡ã¿ãƒãƒƒãƒ—ï¼‰"
        ),
        FieldSchema(
            name="document_id",
            dtype=DataType.VARCHAR,
            max_length=255
        ),
        FieldSchema(
            name="source_type",
            dtype=DataType.VARCHAR,
            max_length=50
        ),
        FieldSchema(
            name="language",
            dtype=DataType.VARCHAR,
            max_length=2
        ),
        FieldSchema(
            name="chunk_position",
            dtype=DataType.INT32
        ),
        FieldSchema(
            name="vocabulary_size",
            dtype=DataType.INT32,
            description="æœ‰åŠ¹èªå½™æ•°"
        ),
        FieldSchema(
            name="created_timestamp",
            dtype=DataType.INT64
        )
    ],
    description="Sparse vectors for keyword search"
)

# Sparse Vectorç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
sparse_index_params = {
    "index_type": "SPARSE_INVERTED_INDEX",
    "metric_type": "IP",
    "params": {
        "drop_ratio_build": 0.2  # ä½é »åº¦èªã®é™¤å¤–ç‡
    }
}
```

### 3. Multi-Vector Collection

```python
# Multi-Vectorï¼ˆColBERTï¼‰ç”¨ã‚¹ã‚­ãƒ¼ãƒ
multi_collection_schema = CollectionSchema(
    fields=[
        FieldSchema(
            name="id",
            dtype=DataType.VARCHAR,
            max_length=255,
            is_primary=True
        ),
        FieldSchema(
            name="vectors",
            dtype=DataType.FLOAT_VECTOR,
            dim=1024,  # å„ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ã‚¯ã‚¿ãƒ¼ã®æ¬¡å…ƒ
            description="ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ãƒ™ã‚¯ã‚¿ãƒ¼ã®é…åˆ—"
        ),
        FieldSchema(
            name="token_count",
            dtype=DataType.INT32,
            description="ãƒˆãƒ¼ã‚¯ãƒ³æ•°"
        ),
        FieldSchema(
            name="token_positions",
            dtype=DataType.JSON,  # ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®æƒ…å ±
            description="å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®æƒ…å ±"
        ),
        FieldSchema(
            name="document_id",
            dtype=DataType.VARCHAR,
            max_length=255
        ),
        FieldSchema(
            name="source_type",
            dtype=DataType.VARCHAR,
            max_length=50
        ),
        FieldSchema(
            name="chunk_position",
            dtype=DataType.INT32
        ),
        FieldSchema(
            name="created_timestamp",
            dtype=DataType.INT64
        )
    ],
    description="Multi-vectors for fine-grained search"
)
```

---

## ğŸ”— ãƒ‡ãƒ¼ã‚¿é–¢é€£ä»˜ã‘ã¨ãƒãƒƒãƒ”ãƒ³ã‚°

### PostgreSQL â†” Milvus é€£æº

```python
@dataclass
class VectorMappingService:
    """PostgreSQLã¨Milvusã®ãƒ‡ãƒ¼ã‚¿é€£æºç®¡ç†"""
    
    async def save_chunk_with_vectors(
        self,
        chunk_data: dict,
        dense_vector: list[float],
        sparse_vector: dict,
        multi_vectors: list[list[float]]
    ) -> str:
        """ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã¨ãƒ™ã‚¯ã‚¿ãƒ¼ã‚’é€£æºä¿å­˜"""
        
        chunk_id = str(uuid.uuid4())
        
        try:
            # 1. PostgreSQLã«ãƒãƒ£ãƒ³ã‚¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            async with self.postgres_pool.acquire() as conn:
                await conn.execute("""
                    INSERT INTO document_chunks 
                    (id, document_id, content, position, token_count, embedding_status)
                    VALUES ($1, $2, $3, $4, $5, 'processing')
                """, chunk_id, chunk_data["document_id"], 
                    chunk_data["content"], chunk_data["position"], 
                    chunk_data["token_count"])
            
            # 2. Milvusã«ãƒ™ã‚¯ã‚¿ãƒ¼ã‚’ä¿å­˜
            vector_ids = await self._save_vectors_to_milvus(
                chunk_id, dense_vector, sparse_vector, multi_vectors
            )
            
            # 3. PostgreSQLã«ãƒ™ã‚¯ã‚¿ãƒ¼IDã‚’æ›´æ–°
            async with self.postgres_pool.acquire() as conn:
                await conn.execute("""
                    UPDATE document_chunks 
                    SET dense_vector_id = $1, sparse_vector_id = $2, 
                        multi_vector_id = $3, embedding_status = 'completed',
                        embedded_at = NOW()
                    WHERE id = $4
                """, vector_ids["dense"], vector_ids["sparse"], 
                    vector_ids["multi"], chunk_id)
            
            return chunk_id
            
        except Exception as e:
            # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            await self._cleanup_failed_chunk(chunk_id)
            raise RuntimeError(f"Chunk saving failed: {e}")
    
    async def _save_vectors_to_milvus(
        self,
        chunk_id: str,
        dense_vector: list[float],
        sparse_vector: dict,
        multi_vectors: list[list[float]]
    ) -> dict[str, str]:
        """Milvusã«ãƒ™ã‚¯ã‚¿ãƒ¼ã‚’ä¿å­˜ã—ã€IDã‚’è¿”å´"""
        
        vector_ids = {}
        
        # Dense Vectorä¿å­˜
        dense_data = {
            "id": f"{chunk_id}_dense",
            "vector": dense_vector,
            "document_id": chunk_id.split("_")[0],
            "created_timestamp": int(time.time())
        }
        await self.milvus_client.insert("dense_collection", [dense_data])
        vector_ids["dense"] = dense_data["id"]
        
        # Sparse Vectorä¿å­˜
        sparse_data = {
            "id": f"{chunk_id}_sparse",
            "vector": sparse_vector,
            "document_id": chunk_id.split("_")[0],
            "vocabulary_size": len(sparse_vector),
            "created_timestamp": int(time.time())
        }
        await self.milvus_client.insert("sparse_collection", [sparse_data])
        vector_ids["sparse"] = sparse_data["id"]
        
        # Multi-Vectorä¿å­˜
        multi_data = {
            "id": f"{chunk_id}_multi",
            "vectors": multi_vectors,
            "token_count": len(multi_vectors),
            "document_id": chunk_id.split("_")[0],
            "created_timestamp": int(time.time())
        }
        await self.milvus_client.insert("multi_collection", [multi_data])
        vector_ids["multi"] = multi_data["id"]
        
        return vector_ids
```

---

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã¨åˆ¶ç´„

### 1. å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã¨å‰Šé™¤ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰

```sql
-- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤æ™‚ã®ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‡¦ç†
CREATE OR REPLACE FUNCTION cleanup_document_vectors()
RETURNS TRIGGER AS $$
DECLARE
    chunk_record RECORD;
BEGIN
    -- å‰Šé™¤ã•ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒ£ãƒ³ã‚¯IDã‚’å–å¾—
    FOR chunk_record IN 
        SELECT dense_vector_id, sparse_vector_id, multi_vector_id 
        FROM document_chunks 
        WHERE document_id = OLD.id
    LOOP
        -- Milvusã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚’éåŒæœŸã§å‰Šé™¤ï¼ˆå¤–éƒ¨é–¢æ•°çµŒç”±ï¼‰
        PERFORM delete_milvus_vectors(
            chunk_record.dense_vector_id,
            chunk_record.sparse_vector_id, 
            chunk_record.multi_vector_id
        );
    END LOOP;
    
    RETURN OLD;
END;
$$ LANGUAGE plpgsql;

-- ãƒˆãƒªã‚¬ãƒ¼è¨­å®š
CREATE TRIGGER trigger_cleanup_document_vectors
    AFTER DELETE ON documents
    FOR EACH ROW
    EXECUTE FUNCTION cleanup_document_vectors();
```

### 2. ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯

```python
class DataConsistencyChecker:
    """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚«ãƒ¼"""
    
    async def check_consistency(self) -> dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        
        inconsistencies = {
            "orphaned_chunks": [],
            "missing_vectors": [],
            "status_mismatches": [],
            "index_issues": []
        }
        
        # 1. å­¤ç«‹ãƒãƒ£ãƒ³ã‚¯ãƒã‚§ãƒƒã‚¯
        orphaned = await self._check_orphaned_chunks()
        inconsistencies["orphaned_chunks"] = orphaned
        
        # 2. æ¬ æãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯  
        missing_vectors = await self._check_missing_vectors()
        inconsistencies["missing_vectors"] = missing_vectors
        
        # 3. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ä¸æ•´åˆãƒã‚§ãƒƒã‚¯
        status_issues = await self._check_status_consistency()
        inconsistencies["status_mismatches"] = status_issues
        
        # 4. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        index_issues = await self._check_index_consistency()
        inconsistencies["index_issues"] = index_issues
        
        return inconsistencies
    
    async def _check_orphaned_chunks(self) -> list[dict]:
        """è¦ªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãªã„ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œå‡º"""
        async with self.postgres_pool.acquire() as conn:
            results = await conn.fetch("""
                SELECT c.id, c.document_id, c.content
                FROM document_chunks c
                LEFT JOIN documents d ON c.document_id = d.id
                WHERE d.id IS NULL
            """)
            return [dict(r) for r in results]
    
    async def _check_missing_vectors(self) -> list[dict]:
        """ãƒ™ã‚¯ã‚¿ãƒ¼ãŒæ¬ æã—ã¦ã„ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œå‡º"""
        async with self.postgres_pool.acquire() as conn:
            results = await conn.fetch("""
                SELECT id, document_id, embedding_status
                FROM document_chunks 
                WHERE embedding_status = 'completed'
                AND (dense_vector_id IS NULL 
                     OR sparse_vector_id IS NULL 
                     OR multi_vector_id IS NULL)
            """)
            return [dict(r) for r in results]
```

---

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥

```sql
-- æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
CREATE TABLE documents_partitioned (
    LIKE documents INCLUDING ALL
) PARTITION BY RANGE (created_at);

-- æœˆåˆ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆ
CREATE TABLE documents_2024_01 PARTITION OF documents_partitioned
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE documents_2024_02 PARTITION OF documents_partitioned
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- è‡ªå‹•ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆé–¢æ•°
CREATE OR REPLACE FUNCTION create_monthly_partition(target_date DATE)
RETURNS void AS $$
DECLARE
    start_date DATE;
    end_date DATE;
    partition_name TEXT;
BEGIN
    start_date := date_trunc('month', target_date);
    end_date := start_date + INTERVAL '1 month';
    partition_name := 'documents_' || to_char(start_date, 'YYYY_MM');
    
    EXECUTE format(
        'CREATE TABLE IF NOT EXISTS %I PARTITION OF documents_partitioned
         FOR VALUES FROM (%L) TO (%L)',
        partition_name, start_date, end_date
    );
END;
$$ LANGUAGE plpgsql;
```

### 2. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–

```sql
-- è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹æ¤œç´¢æ¡ä»¶ï¼‰
CREATE INDEX idx_chunks_search_optimized 
    ON document_chunks (embedding_status, document_id, position)
    WHERE embedding_status = 'completed';

-- éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚½ãƒ¼ã‚¹ã®ã¿ï¼‰
CREATE INDEX idx_sources_active_sync 
    ON sources (next_sync_at, source_type)
    WHERE is_active = true;

-- ä¸¦è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰
CREATE INDEX CONCURRENTLY idx_documents_fulltext_gin 
    ON documents USING GIN (to_tsvector('japanese', title || ' ' || content));
```

### 3. Milvus ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š

```python
# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆæ™‚ã®æœ€é©åŒ–è¨­å®š
collection_config = {
    "shards_num": 2,          # ã‚·ãƒ£ãƒ¼ãƒ‰æ•°
    "consistency_level": "Strong",  # æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«
}

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
index_build_config = {
    "index_type": "HNSW",
    "metric_type": "IP",
    "params": {
        "M": 16,                    # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ã®ãƒãƒ©ãƒ³ã‚¹
        "efConstruction": 256,       # æ§‹ç¯‰æ™‚é–“ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹
        "max_memory_usage": "2GB"    # ãƒ¡ãƒ¢ãƒªåˆ¶é™
    }
}

# æ¤œç´¢æ™‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
search_config = {
    "params": {
        "ef": 128,              # æ¤œç´¢ç²¾åº¦
        "nprobe": 16            # ä¸¦è¡Œæ¤œç´¢æ•°
    },
    "limit": 100,              # å–å¾—ä»¶æ•°ä¸Šé™
    "timeout": 30              # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
}
```

---

## â— ã‚ˆãã‚ã‚‹è½ã¨ã—ç©´ã¨å¯¾ç­–

### 1. ãƒ™ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒä¸ä¸€è‡´

```python
# âŒ å•é¡Œ: ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã®æ¬¡å…ƒæ•°æ··åœ¨
def save_vector_unsafe(vector: list[float]):
    # æ¬¡å…ƒãƒã‚§ãƒƒã‚¯ãªã—ã§ä¿å­˜ â†’ æ¤œç´¢æ™‚ã‚¨ãƒ©ãƒ¼
    collection.insert([{"id": "test", "vector": vector}])

# âœ… å¯¾ç­–: äº‹å‰æ¬¡å…ƒãƒã‚§ãƒƒã‚¯
def save_vector_safe(vector: list[float]):
    EXPECTED_DIM = 1024
    if len(vector) != EXPECTED_DIM:
        raise ValueError(
            f"Vector dimension mismatch: expected {EXPECTED_DIM}, "
            f"got {len(vector)}"
        )
    
    # æ­£è¦åŒ–ã‚‚å®Ÿæ–½
    normalized_vector = normalize_vector(vector)
    collection.insert([{"id": "test", "vector": normalized_vector}])
```

### 2. ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å¢ƒç•Œã®å•é¡Œ

```python
# âŒ å•é¡Œ: åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ã®ä¸æ•´åˆ
async def save_document_unsafe(doc_data, vectors):
    # PostgreSQLä¿å­˜
    doc_id = await postgres_repo.save(doc_data)
    
    # Milvusä¿å­˜ï¼ˆå¤±æ•—æ™‚ã€PostgreSQLãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã‚‹ï¼‰
    await milvus_client.insert(vectors)

# âœ… å¯¾ç­–: Sagaãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹åˆ†æ•£ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³
async def save_document_safe(doc_data, vectors):
    compensation_actions = []
    
    try:
        # 1. PostgreSQLä¿å­˜
        doc_id = await postgres_repo.save(doc_data)
        compensation_actions.append(
            lambda: postgres_repo.delete(doc_id)
        )
        
        # 2. Milvusä¿å­˜
        vector_ids = await milvus_client.insert(vectors)
        compensation_actions.append(
            lambda: milvus_client.delete(vector_ids)
        )
        
        # 3. ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±æ›´æ–°
        await postgres_repo.update_vector_ids(doc_id, vector_ids)
        
        return doc_id
        
    except Exception as e:
        # é€†é †ã§è£œå„Ÿå‡¦ç†å®Ÿè¡Œ
        for action in reversed(compensation_actions):
            try:
                await action()
            except Exception as cleanup_error:
                logger.error(f"Cleanup failed: {cleanup_error}")
        raise e
```

### 3. ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã¨ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†

```python
# âœ… é©åˆ‡ãªãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
class DatabaseManager:
    def __init__(self):
        self._postgres_pool = None
        self._milvus_client = None
        self._redis_client = None
    
    async def __aenter__(self):
        # ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–
        self._postgres_pool = await asyncpg.create_pool(
            dsn=DATABASE_URL,
            min_size=5,
            max_size=20,
            command_timeout=60
        )
        
        self._milvus_client = MilvusClient(
            host=MILVUS_HOST,
            port=MILVUS_PORT,
            pool_size=10
        )
        
        self._redis_client = await aioredis.from_url(
            REDIS_URL,
            max_connections=20
        )
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        # é©åˆ‡ãªãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
        if self._postgres_pool:
            await self._postgres_pool.close()
        
        if self._milvus_client:
            await self._milvus_client.close()
        
        if self._redis_client:
            await self._redis_client.close()
```

---

## ğŸ¯ ç†è§£ç¢ºèªã®ãŸã‚ã®è¨­å•

### ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆç†è§£
1. `documents`ãƒ†ãƒ¼ãƒ–ãƒ«ã§`processing_status`ã¨`indexing_status`ã‚’åˆ†ã‘ã¦ã„ã‚‹ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
2. `document_chunks`ãƒ†ãƒ¼ãƒ–ãƒ«ã®`parent_chunk_id`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç”¨é€”ã¨éšå±¤æ§‹é€ ã®è¡¨ç¾æ–¹æ³•ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
3. Milvusã§Denseã€Sparseã€Multi-Vectorã‚’åˆ¥ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†ã‘ã‚‹ãƒ¡ãƒªãƒƒãƒˆã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„

### ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç†è§£
1. PostgreSQL-Milvusé–“ã®ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’ä¿ã¤ãŸã‚ã«å®Ÿè£…ã•ã‚ŒãŸä»•çµ„ã¿ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
2. åˆ†æ•£ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã§Sagaãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ç†ç”±ã¨è£œå„Ÿå‡¦ç†ã®é‡è¦æ€§ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
3. `cleanup_document_vectors()`ãƒˆãƒªã‚¬ãƒ¼é–¢æ•°ãŒå¿…è¦ãªç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç†è§£
1. å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãŒæœ‰åŠ¹ãªç†ç”±ã¨ã€é©åˆ‡ãªåˆ†å‰²æˆ¦ç•¥ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
2. Milvusã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆMã€efConstructionï¼‰ã®èª¿æ•´æŒ‡é‡ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
3. éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã®åˆ©ç‚¹ã¨é©ç”¨å ´é¢ã‚’èª¬æ˜ã—ã¦ãã ã•ã„

### é‹ç”¨ç†è§£
1. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚«ãƒ¼ã§æ¤œå‡ºã™ã¹ã4ç¨®é¡ã®ä¸æ•´åˆã¨ãã®å½±éŸ¿ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
2. ãƒ™ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒä¸ä¸€è‡´ãŒç™ºç”Ÿã™ã‚‹åŸå› ã¨äº‹å‰é˜²æ­¢ç­–ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
3. ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«è¨­å®šã§è€ƒæ…®ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’5ã¤æŒ™ã’ã¦ãã ã•ã„

---

## ğŸ“š æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã‚’ç†è§£ã§ããŸã‚‰ã€æ¬¡ã®å­¦ç¿’æ®µéšã«é€²ã‚“ã§ãã ã•ã„ï¼š

- **Step06**: èªè¨¼ãƒ»èªå¯ã‚·ã‚¹ãƒ†ãƒ  - JWTãƒ»API Keyèªè¨¼ã®å®Ÿè£…è©³ç´°
- **Step07**: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ç›£è¦– - ä¾‹å¤–å‡¦ç†ãƒ»ãƒ­ã‚°ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
- **Step08**: ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¨é‹ç”¨ - Dockerãƒ»Kubernetesãƒ»CI/CD

é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã¯ã€ã‚·ã‚¹ãƒ†ãƒ ã®æ‹¡å¼µæ€§ã¨ä¿å®ˆæ€§ã‚’æ±ºå®šã™ã‚‹é‡è¦ãªè¦ç´ ã§ã™ã€‚æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ä¿è­·ã™ã‚‹èªè¨¼ãƒ»èªå¯ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦å­¦ç¿’ã—ã¾ã™ã€‚